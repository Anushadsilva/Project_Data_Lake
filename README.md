Data lake using Spark
Sparkify is a music streaming startup that has grown really fast for the past few months and now its services is known world wide.

We are to build an ETL pipeline that extracts the data from S3, processes it using Spark, and loads the data back into S3 as a set of dimensional tables. I have written a script in python that uses Spark SQL and Spark Data Frames to execute the following:

Create a Spark session using the Apache Hadoop Amazon Web Services Support module
load AWS access keys into environment variables
ingest log files and song data files from S3
clean and process the data:
add unique row identifiers to all fact and dimension tables
parse timestamps into time and date components
create tables for
write the final set of tables to S3


How to Use
Run etl.py from terminal or python console.

Files in Repository
The tables produced by the ETL process are called songplays, users, artists, songs and time. Each of the five tables are written to parquet files in a separate analytics directory on S3. Each table has its own folder within the directory. Songs table files are partitioned by year and then artist. Time table files are partitioned by year and month. Songplays table files are partitioned by year and month.

Fact Table
songplays - records in log data associated with song plays i.e. records with page NextSong

songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
Dimension Tables
users - users in the app

user_id, first_name, last_name, gender, level
songs - songs in music database

song_id, title, artist_id, year, duration
artists - artists in music database

artist_id, name, location, lattitude, longitude
time - timestamps of records in songplays broken down into specific units start_time, hour, day, week, month, year, weekday


s3a://udacity-dend/song_data : Contains metadata about a song and the artist of that song;
s3a://udacity-dend/log_data : Consists of log files generated by the streaming app based on the songs in the dataset above;

Logs data processing
The data will be populated from logs_data folder in s3, and uses the songs and artists parquet files to create the fact table

Usage:
Connect to your master using scp -i ~/yourkeypair.pem etl.py hadoop@ec2-3-21-129-236.us-east-2.compute.amazonaws.com:/home/hadoop/ ssh -i ~/yourkeypair.pem hadoop@ec2-3-21-129-236.us-east-2.compute.amazonaws.com spark-submit etl.py


etl.py reads data from S3, processes that data using Spark, and writes them back to S3
dl.cfgcontains your AWS credentials
README.md provides discussion on your process and decisions

Files written on S3: Artist , Songplay, users , Songs , time 
